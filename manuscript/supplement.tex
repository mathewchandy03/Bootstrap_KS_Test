\documentclass[12pt]{article}

\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{authblk}
\usepackage{algorithm}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{algpseudocode}
\usepackage{bbm}
\usepackage{bm}
\usepackage{caption}
\usepackage{color}
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{float}
\usepackage{mathtools}
\usepackage{makecell}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage{natbib}
\usepackage{comment}
\usepackage{longtable}
\usepackage{rotating}
\usepackage{xcolor}
\usepackage{fullpage}
\usepackage[utf8]{inputenc}

\newcommand{\mc}[1]{\textcolor{blue}{MC: (#1)}}
\newcommand{\jy}[1]{\textcolor{purple}{JY: (#1)}}

\title{Supporting Information for
  ``Nonparametric Block Bootstrap Kolmogorov--Smirnov Goodness-of-Fit
  Test''}
\author{Mathew Chandy, Elizabeth D. Schifano, Jun Yan, and Xianyang
  Zhang}


\begin{document}
\maketitle


\section{Asymptotic Limit of the KS Statistic}

\begin{proof}[Proof of Proposition~1]
We first derive the limiting distribution of the KS statistic under the null. 
Suppose $\hat{\theta}_n$ admits the expansion:
\begin{align}\label{eq-expan}
\hat{\theta}_n-\theta_0 = \frac{1}{n}\sum^{n}_{i=1}h(X_i;\theta_0) + 
\frac{1}{\sqrt{n}}R_n,    
\end{align}
where $h(\cdot;\theta_0)$ is the influence function with 
$\mathbb{E}[h(X_i;\theta_0)]=0$ and $R_n$ is the remainder term satisfying that
$R_n\overset{p}{\rightarrow} 0.$ Write $G_n(x):=\sqrt{n}(F_n(x) - 
F(x; \theta_0))$ 
and let $f(x;\theta)=\partial F(x;\theta)/\partial \theta$. We assume 
$\theta \in \mathbb{R}$ for ease of presentation.

\paragraph{Finite-dimensional convergence of $G_n(x_1),\dots,G_n(x_m)$}

We assume that the sequence \(\{X_i\}\) is strictly stationary and satisfies a 
strong mixing condition. This condition ensures that the dependence between 
\(X_i\) and \(X_{i+h}\) weakens sufficiently fast as \(h\) increases.


We aim to show that for any fixed \( x_1, \ldots, x_m \), the vector
\( (G_n(x_1), \ldots, G_n(x_m)) \) converges in distribution to a
multivariate normal vector \( (G_{\infty}(x_1), \ldots,
G_{\infty}(x_m)) \) with mean zero and covariance structure given by:
\[
  \text{Cov}(G_{\infty}(x_i), G_{\infty}(x_j)) =
  \sum_{h=-\infty}^{\infty}
  \text{Cov}(\mathbf{1}\{X_0 \leq x\}, \mathbf{1}\{X_h \leq x^{\prime}\}),
\]
where $ \mathbf{1}\{\cdot\}$ is the indicator
function.


Given any $x_1,\dots,x_m$ in the domain of $F$, by the CLT for weakly dependent 
time series, we have the joint convergence of 
$V_n:=(G_n(x_1),\dots,G_n(x_m),\sqrt{n}(\hat{\theta}_n-\theta_0))$
to some Gaussian limit, with the covariance matrix being 
$\Sigma_{m+1}=\lim_{n\rightarrow+\infty}\text{Cov}(V_n)$.
Consider the empirical process $ G_n(x) = \sqrt{n} (F_n(x) - F(x)) $, where 
$ F_n(x) $ is the empirical distribution function based on $ X_1, \ldots, 
X_n $. To show finite-dimensional convergence, we need to establish that for 
any fixed $ x_1, \ldots, x_m $, the vector $ (G_n(x_1), \ldots, G_n(x_m)) $ 
converges in distribution to a multivariate normal distribution.
The empirical distribution function can be written as
$
F_n(x) = \frac{1}{n} \sum_{t=1}^n \mathbf{1}\{X_i \leq x\}.
$
Therefore, the empirical process is
\[
G_n(x) = \sqrt{n} (F_n(x) - F(x)) = \frac{1}{\sqrt{n}} \sum_{i=1}^n 
(\mathbf{1}\{X_i \leq x\} - F(x)).
\]


To establish the asymptotic distribution of $ G_n(x) $, we consider the vector 
\[
  (G_n(x_1), \ldots, G_n(x_m)) =
  \frac{1}{\sqrt{n}} \sum_{i=1}^n
  \left(\mathbf{1}\{X_i \leq x_1\} - 
    F(x_1), \ldots, \mathbf{1}\{X_i \leq x_m\} - F(x_m)\right).
\]
We calculate the covariance structure of the limiting distribution. The 
covariance between $ G_n(x) $ and $ G_n(x^{\prime}) $ is:
\[
\text{Cov}(G_n(x), G_n(x^{\prime})) = \frac{1}{n} \sum_{t=1}^n \sum_{s=1}^n 
\text{Cov}\left(\mathbf{1}\{X_t \leq x\}, \mathbf{1}\{X_t \leq x^{\prime}\}\right).
\]
Since $ \{X_i\} $ is strictly stationary, the covariance depends only on the 
lag $ h = |t-s| $:
\[
\text{Cov}(\mathbf{1}\{X_t \leq x\}, \mathbf{1}\{X_t \leq x^{\prime}\}) = \text{Cov}(\mathbf{1}\{X_h \leq x\}, 
\mathbf{1}\{X_0 \leq x^{\prime}\}).
\]
Thus, the covariance can be expressed as
\[
\text{Cov}(G_n(x), G_n(x')) = \frac{1}{n} \sum_{t=1}^n \sum_{h=-t+1}^{n-t} 
\text{Cov}(\mathbf{1}\{X_h \leq x\}, 
\mathbf{1}\{X_0 \leq x^{\prime}\}).
\]


By the mixing condition, the covariances
$ \text{Cov}(\mathbf{1}\{X_h \leq x\}, \mathbf{1}\{X_0 \leq x^{\prime}\}) $ 
decay sufficiently fast as $ h \to \infty $. Therefore, the sum converges to:
\[
  \lim_{n \to \infty} \text{Cov}(G_n(x), G_n(x'))
  = \sum_{h=-\infty}^{\infty} \text{Cov}(\mathbf{1}\{X_h \leq x\}, 
  \mathbf{1}\{X_0 \leq x^{\prime}\}).
\]


\paragraph{Tightness of $G_n(x)$}

Moreover, we can show the tightness of the process $G_n(s)$. 
We want to show that for every $\epsilon > 0$,
\[
\lim_{\delta \to 0} \limsup_{n \to \infty} \mathbb{P}\left( \omega_{G_n}(\delta) 
\geq \epsilon \right) = 0,
\]
where the modulus of continuity $\omega_{G_n}(\delta)$ is defined as
\citep{billingsley2013convergence}.
\[
\omega_{G_n}(\delta) = \sup_{|s - t| < \delta} |G_n(s) - G_n(t)|.
\]
For sufficiently small $\delta$, and leveraging the properties of weakly 
dependent sequences, we can show that
\[
  \sup_{|s - t| < \delta} |G_n(s) - G_n(t)|
  = \sup_{|s - t| < \delta}  \sqrt{n}|(F_n(s) - F(s)) - (F_n(t) - F(t))|
\]
is small with high probability.
For $ |s - t| < \delta $,
\[
|G_n(s) - G_n(t)| = \sqrt{n} \left| (F_n(s) - F(s; \theta_0)) - (F_n(t) - 
F(t; \theta_0)) \right|.
\]
This can be decomposed as:
\[
|G_n(s) - G_n(t)| = \sqrt{n} \left| (F_n(s) - F_n(t)) - (F(s; \theta_0) - 
F(t; \theta_0)) \right|.
\]


Remember that $F_n(t) = \sum_{i=1}^n  \mathbf{1}\{X_i \le t\} / n$. Then, for 
$ |s - t| < \delta $, the difference $ F_n(s) - F_n(t) $ can be written as
\[
  F_n(s) - F_n(t) = \frac{1}{n}
  \sum_{i=1}^n \left( \mathbf{1}\{X_i \leq s\} - \mathbf{1}\{X_i \leq t\} \right),
\]
or as 
$ \mathbb{E}\left[ \frac{1}{n} \sum_{i=1}^n \mathbf{1}\{ t < X_i \leq s \} 
\right] $. To provide a more precise bound on $ F_n(s) - F_n(t) $, we use 
Hoeffding's inequality, which states that for any $ t > 0 $,
\[
  \mathbb{P}\left( \left| \frac{1}{n} \sum_{i=1}^n \mathbf{1}\{ t < X_i \leq s \} - 
      \mathbb{E}\left[ \frac{1}{n} \sum_{i=1}^n \mathbf{1}\{ t < X_i \leq s \} 
      \right] \right| > t \right) \leq 2 \exp\left( -2nt^2 \right).
\]


Since 
\[ \mathbb{E}\left[ \frac{1}{n} \sum_{i=1}^n \mathbf{1}\{ t < X_i \leq s \} 
\right] = F(s; \theta_0) - F(t; \theta_0),
\]
we have:
\[
  \mathbb{P}\left( \left| \frac{1}{n} \sum_{i=1}^n \mathbf{1}\{ t < X_i \leq s \} - 
      (F(s; \theta_0) - F(t; \theta_0)) \right| > t \right) \leq
  2\exp\left( -2nt^2 \right).
\]
Because the true CDF is continuous, by definition,
$ F(s; \theta_0) $, for any $ \epsilon > 0 $,
there exists a $ \delta > 0 $ such that for $ |s - t| < \delta $,
$ |F(s; \theta_0) - F(t; \theta_0)| < \epsilon. $
Using the bound from Hoeffding's inequality, we have:
\[
  \mathbb{P}\left( |F_n(s) - F_n(t) - (F(s; \theta_0) - F(t; \theta_0))| \geq 
    \epsilon \right) \leq 2 \exp\left( -2n\epsilon^2 \right).
\]
Then, for sufficiently large $ n $ and small $ \delta $,
$ \mathbb{P}\left( \omega_{G_n}(\delta) \geq \epsilon \right) \leq 2 
\exp\left( -2n\epsilon^2 \right). $ Taking limits, we conclude:
$ \lim_{\delta \to 0} \limsup_{n \to \infty} \mathbb{P}\left( 
\omega_{G_n}(\delta) \geq \epsilon \right) = 0. $

\paragraph{Convergence in distribution}
Therefore, we have
\begin{align*}
\begin{pmatrix}
G_n(x)  \\ 
\sqrt{n}(F(x;\hat{\theta}_n)-F(x;\theta_0))
\end{pmatrix}\Rightarrow
\begin{pmatrix}
G_{\infty}(x) \\
f(x;\theta_0)U
\end{pmatrix}
\end{align*}
jointly, where $G_{\infty}(x)$ is a Gaussian process with the covariance 
function defined as
\begin{align*}
\text{Cov}(G_{\infty}(x),G_{\infty}(x'))=& 
\lim_{n\rightarrow+\infty}\frac{1}{n}\sum^{n}_{i,j=1}
\text{Cov}(\mathbf{1}\{X_i\leq x\},\mathbf{1}\{X_j\leq x'\})
\\=& \sum^{+\infty}_{h=-\infty}\text{Cov}(\mathbf{1}\{X_{h}\leq x\},
\mathbf{1}\{X_0\leq x'\}),
\end{align*}
and $U\sim N(0, s^2)$ with 
\begin{align*}
s^2=\lim_{n\rightarrow+\infty}\frac{1}{n}\sum^{n}_{i,j=1}\text{Cov}(h(X_i,
\theta_0),h(X_j;\theta_0))
= \sum^{+\infty}_{h=-\infty}\text{Cov}(h(X_{h},\theta_0),h(X_0;\theta_0)),
\end{align*}
and the covariance between $U$ and $G_{\infty}(x)$ is given by
\begin{align*}
\text{Cov}(U,G_\infty(x))=&\lim_{n\rightarrow+\infty}
\frac{1}{n}\sum^{n}_{i,j=1}\text{Cov}(h(X_i,\theta_0),\mathbf{1}\{X_j\leq x\})
\\=&\sum^{+\infty}_{h=-\infty}
\text{Cov}(h(X_{h},\theta_0),\mathbf{1}\{X_0\leq x\}).
\end{align*}
By the continuous mapping theorem, we obtain
\begin{align*}
T_n \overset{d}{\rightarrow} \sup_x |G_{\infty}(x)+f(x;\theta_0)U|.    
\end{align*}

\end{proof}

\section{Bootstrap Consistency}

Denote by $\mathbb{E}^{(b)}$, $\text{Var}^{(b)}$ and $\text{Cov}^{(b)}$
the expectation, variance and covariance conditional on 
$\{X_i: i = 1, \ldots, n\}$ in 
the bootstrap world. Assuming that $n=kl$ with $l$ being the block size and $k$ 
being the number of blocks, the bootstrap marginal distribution is 
\begin{align*}
\rho_n^{(b)}=\frac{1}{n}\sum^{k}_{j=1}\sum^{S_j+l-1}_{i=S_j} \delta_{X_i},    
\end{align*}
where $\delta_x$ denotes a point mass at $x$, and $S_1,\dots,S_k$ are i.i.d 
uniform on $\{1,\dots,n-l+1\}$. 
Define $G_n^{(b)}(x)$ in the same way as $G_n(x)$ with $X_1, \dots, X_n$ being 
replaced by the bootstrap samples $X_1^{(b)},\dots,X_n^{(b)}.$


\begin{proof}[Proof of Proposition~2]
  We want to show that
  \jy{What is $\hat\theta_0$? Should it be $\hat\theta_n$?}
\begin{align}\label{eq-con-boot}
\begin{pmatrix}
G_n^{(b)}(x)  \\ 
\sqrt{n}(F(x;\hat{\theta}_n^{(b)})-F(x;\hat{\theta}_0))
\end{pmatrix}\Rightarrow^{*}
\begin{pmatrix}
G_{\infty}(x) \\
f(x;\theta_0)U
\end{pmatrix}
\end{align}
where $\Rightarrow^*$ means convergence in distribution almost surely and 
$\hat{\theta}_0:=\mathbb{E}^{(b)}[\hat{\theta}_n^{(b)}]$. 
In the bootstrap world, we have an expansion similar to (\ref{eq-expan}):
\begin{align*}
\hat{\theta}_n^{(b)}-\hat{\theta}_0 = \frac{1}{n}\sum^{n}_{i=1}h(X_i^{(b)};
\hat{\theta}_0) + \frac{1}{\sqrt{n}}R_n^{(b)},    
\end{align*}
where $R_n^{(b)} \overset{p^*}{\rightarrow} 0$, where 
$\overset{p^*}{\rightarrow}$ means convergence in probability almost surely. 
Note that 
\begin{align*}
\mathbb{E}^{(b)}\left[\frac{1}{n}\sum^{n}_{i=1}h(X_i^{(b)};
\hat{\theta}_0)\right]
=&\frac{1}{n}\sum^{k}_{j=1}\mathbb{E}^{(b)}\left[\sum^{S_j+l-1}_{i=S_j}h(X_{i};
\hat{\theta}_0)\right]
\\=&\frac{k}{n^2}[\sum^{n - l + 1}_{j=1}\sum^{j+l-1}_{i=j}h(X_{i};
\hat{\theta}_0) +
\sum^{n}_{j=n - l + 2}[\sum^{n}_{i=j}h(X_{i};\hat{\theta}_0) +
\sum^{j - n + l -1}_{i=1}h(X_{i};\hat{\theta}_0)]]
\\=&\frac{k}{n^2}\sum^{n}_{i=1}l h(X_{i};\hat{\theta}_0).
\end{align*}


Moreover, we have
\begin{align*}
\text{var}^{(b)}\left(\frac{1}{\sqrt{n}}\sum^{n}_{i=1}h(X_i^{(b)};
\theta_0^{(b)})\right)
=&\text{var}^{(b)}\left(\frac{1}{\sqrt{n}}\sum^{k}_{j=1}\sum^{S_j+l-1}_{i=S_j}
h(X_{i};\hat{\theta}_0)\right)
\\=&\frac{1}{l}\text{var}^{(b)}\left(\sum^{S_1+l-1}_{i=S_1}h(X_{i};
\hat{\theta}_0)\right)
\\=&\frac{1}{ln}\sum^{n}_{t=1}(W_t-\bar{W})^2
\end{align*}
with
\[
  W_t=
 \begin{cases}
\sum^{t+l-1}_{i=t}h(X_i;\hat{\theta}_0)  , & \text{if } t \leq n - 1 + 1,\\
\sum^{n}_{i=t}h(X_{i};\hat{\theta}_0) +
\sum^{t - n + l -1}_{i=1}h(X_{i};\hat{\theta}_0) , & \text{if } t \geq n - 1 + 2,
\end{cases}
\]
and $\bar{W}=\sum^{n}_{t=1}W_t/n$,
which is expected to converge to $s^2$ almost surely.


Similar calculations apply 
to $G_n^{(b)}(x)$ and we expect that
\begin{align*}
&\text{Cov}^{(b)}(G_n^{(b)}(x),G_n^{(b)}(x'))\overset{a.s.}{\rightarrow}   
\sum^{+\infty}_{h=-\infty}
\text{Cov}(\mathbf{1}\{X_{h}\leq x\},\mathbf{1}\{X_0\leq x'\}),\\
&\text{Cov}^{(b)}\left(\frac{1}{\sqrt{n}}\sum^{n}_{i=1}h(X_i^{(b)};
\hat{\theta}_0), G_n^{(b)}(x)\right)\overset{a.s.}{\rightarrow} 
\sum^{+\infty}_{h=-\infty}\text{Cov}(h(X_{h},\theta_0),\mathbf{1}\{X_0\leq x\}).
\end{align*}
By showing the finite-dimensional convergence and tightness in the bootstrap 
world (conditional on the data $\{X_i: i = 1, \ldots, n\}$), we can establish 
\eqref{eq-con-boot}.
From the CLT, the process $G_n^{(b)}(x) = \sqrt{n}(F_n^{(b)}(x) - F_n(x))$ 
converges  to $G_{\infty}(x)$.

\paragraph{Finite-Dimensional Convergence in the Bootstrap World}

For $\sqrt{n}(F(x;\hat{\theta}_n^{(b)}) - F(x;\hat{\theta}_0))$, using the 
first-order Taylor 
expansion around $\hat{\theta}_0$ similarly to what was done in 
\citet{kunsch1989jackknife}, we have:
$
F(x;\hat{\theta}_n^{(b)}) \approx F(x;\hat{\theta}_0) + f(x; 
\hat{\theta}_0) (\hat{\theta}_n^{(b)} - \hat{\theta}_0),
$ so
\begin{align*}
  \sqrt{n}(F(x;\hat{\theta}_n^{(b)}) - F(x;\hat{\theta}_0))
  &\approx \sqrt{n} (F(x;\hat{\theta}_0) +
  f(x; \hat{\theta}_0) (\hat{\theta}_n^{(b)} - \hat{\theta}_0) - F(x;\hat{\theta}_0)) \\
  &\approx  f(x; \hat{\theta}_0) \sqrt{n} (\hat{\theta}_n^{(b)} - \hat{\theta}_0).
\end{align*}


Substituting the expansion of $\hat{\theta}_n^{(b)}$:
\[
\sqrt{n} (\hat{\theta}_n^{(b)} - \hat{\theta}_0) = \frac{1}{\sqrt{n}} 
\sum_{i=1}^{n} 
h(X_i^{(b)}; \hat{\theta}_0) + R_n^{(b)}.
\]

Since $R_n^{(b)} \overset{p^{(b)}}{\rightarrow} 0$, 
\[
\sqrt{n}(F(x;\hat{\theta}_n^{(b)}) - F(x;\hat{\theta}_0)) \approx 
f(x; \hat{\theta}_0) \left( \frac{1}{\sqrt{n}} \sum_{i=1}^{n} h(X_i^{(b)}; 
\hat{\theta}_0) \right).
\]
The term $\frac{1}{\sqrt{n}} \sum_{i=1}^{n} h(X_i^{(b)}; \hat{\theta}_0)$ 
converges in distribution to a normal random variable $U$ with variance 
$s^2$.


\paragraph{Tightness in Bootstrap World}


\end{proof}

\bibliographystyle{chicago} 
\bibliography{citations}


\end{document}

