\documentclass[12pt, letterpaper]{article}
%\documentclass[12pt, letterpaper, titlepage]{article}

\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\hypersetup{colorlinks = true, linkcolor = blue, citecolor=blue, urlcolor = blue}
\usepackage{natbib}
\usepackage{enumitem}
\usepackage{setspace}

\usepackage[]{lineno}
\linenumbers*[1]
% %% patches to make lineno work better with amsmath
\newcommand*\patchAmsMathEnvironmentForLineno[1]{%
 \expandafter\let\csname old#1\expandafter\endcsname\csname #1\endcsname
 \expandafter\let\csname oldend#1\expandafter\endcsname\csname end#1\endcsname
 \renewenvironment{#1}%
 {\linenomath\csname old#1\endcsname}%
 {\csname oldend#1\endcsname\endlinenomath}}%
\newcommand*\patchBothAmsMathEnvironmentsForLineno[1]{%
 \patchAmsMathEnvironmentForLineno{#1}%
 \patchAmsMathEnvironmentForLineno{#1*}}%

\AtBeginDocument{%
 \patchBothAmsMathEnvironmentsForLineno{equation}%
 \patchBothAmsMathEnvironmentsForLineno{align}%
 \patchBothAmsMathEnvironmentsForLineno{flalign}%
 \patchBothAmsMathEnvironmentsForLineno{alignat}%
 \patchBothAmsMathEnvironmentsForLineno{gather}%
 \patchBothAmsMathEnvironmentsForLineno{multline}%
}

% control floats
\renewcommand\floatpagefraction{.9}
\renewcommand\topfraction{.9}
\renewcommand\bottomfraction{.9}
\renewcommand\textfraction{.1}
\setcounter{totalnumber}{50}
\setcounter{topnumber}{50}
\setcounter{bottomnumber}{50}

\newcommand{\jy}[1]{\textcolor{blue}{JY: #1}}
\newcommand{\eds}[1]{\textcolor{red}{EDS: (#1)}}
\newcommand{\mc}[1]{\textcolor{green}{MC: (#1)}}

% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{0]}

\begin{document}
%\maketitle

\title{\bf Nonparametric Bootstrap Kolmogorov-Smirnov Test for
  Goodness-of-Fit of Marginal Distributions of Stationary Time Series}
\if0\blind
{
  \author{Mathew Chandy, %\\
%   \href{mailto:mathew.chandy@uconn.edu}
%   {\nolinkurl{mathew.chandy@uconn.edu}}\\
  Elizabeth Schifano\\
  Jun Yan, %\\
  Xianyang Zhang\\
}
} \fi


\maketitle


\begin{abstract}

The Kolmogorov-Smirnov (KS) statistic is widely used to test if a sample is
from a given distribution. This study demonstrates how block bootstrap can be 
used to approximate the KS statistic in situations where parameters are 
not specified and the data are serially dependent. We demonstrate the 
theoretical justication for this method, then through simulation we verify that
it holds it size under the null distribution and is powerful when the null 
hypothesis is false. Finally, we demonstrate applications of the method to
precipitation data from JFK airport and Microsoft stock returns.

\bigskip
\noindent{\sc Keywords}:
\end{abstract}

\doublespace 


\section{Introduction}
\label{sec:intro}


\jy{Flow:
  Review misuses of one-sample KS test; focus on serial dependence;
  criticize Zeimbakakis; highlight contributions (connection to Babu and Rao).}

\jy{Borrow the introduction of Zeimbakakis and, of course, rephase them.}

The Kolmogorov-Smirnov (KS) test is a useful goodness-of-fit statistic. It 
can be used to see if a population follows a hypothesized distribution and has 
and can be applied to a variety of fields. It has
been used to analyze the random distribution of cosmic microwave background 
radiation \citep{naess2012application}. A common misuse of the KS test is when
the hypothesized distribution has unspecified parameters. 
\citet{babu2004goodness} approaches this scenario using basic 
non-parametric bootstrap. There is also the scenario where both the hypothesized 
distribution has unspecified parameters and additionally the data are 
serially dependent. \citet{zeimbekakis2022misuses} demonstrates a remedy for
this scenario using semi-parametric bootstrap. This study aims to address this
scenario with non-parametric block bootstrap.


The rest of the paper is outlined as follows: in the first 
section, we assess whether the KS
test holds its size; that is, does the test fail to reject the null hypothesis
when it is true. In the second section, we evaluate the 
test's power; that is, does the test reject the null hypothesis when it is 
false.
Finally, we end with 
concluding remarks in the third section.


\section{Methods}
\label{sec:methods}

\jy{Clarify the GoF test is for the marginal distribution of a statoinary
  series.}
Consider a stationary time series $\{X_t: t = 1, \ldots, n\}$ with length~$n$.
\jy{state the null hypothesis explicitly; we are testing the marginal
  distribution. Point out the challenges.}
Let $H_0: X_t ~ F$, where if $F$ is a hypothesized distribution with unknown 
parameters. Let $H_A$ be that $X_t$ does not follow $F$. This is challenging because we are
dealing with a situation with both unknown parameters and temporal dependence.
If $X_t ~ F$, the true marginal distribution of $X_t$ has some parameters 
$\theta$.



\jy{No double letter notations}
\mc{addressed}
Consider the goodness of fit statistic:
\jy{define $Y_n$ before introducing $T_n$}
\begin{equation*}
  T_n := \sup_x|Y_n(x; \hat\theta)|, 
Y_n(x; \hat\theta) = \sqrt{n}(F_n(x) - F(x; \hat\theta_n)),
\end{equation*}
where $F_n$ denotes the empirical distribution function based on $X_1,...,X_n$,
and $\hat\theta_n$ denotes the fitted parameters for the hypothesized 
distribution fitted onto $X_t$.
We note that
\begin{equation*}
Y_n(x; \hat\theta) = \sqrt{n}(F_n(x) - F(x)) - 
\sqrt{n}(F(x; \hat\theta_n) - F(x)),
\end{equation*}
where $F(x)$ is the true cdf (under the null $F(x) = F(x, \theta_0)$ for some
true parameter $\theta_0$).

\jy{respond to my comments}

Let us first consider the case where $X_i$'s are i.i.d. Denote by $F^{(b)}_n$ the
empirical distribution of the $b$th bootstrap sample and let
$\hat\theta^{(b)}_n$ be the parameter estimate based on the bootstrap samples. 
Using the bootstrap (asymptotic) theory, we can approximate the distribution of
\jy{Need rephrasing. The two terms are approximated altogether jointly not
  separately.}
$\sqrt{n}(F_n(x) - F(x))$
by that of $\sqrt{n}(F^{(b)}_n(x) - F_n(x))$
and the distribution of
$\sqrt{n}(F(x; \hat\theta_n) - F(x))$
by that of
$\sqrt{n}(F(x; \hat\theta^{(b)}_n) - F(x; \hat\theta_n))$.
Therefore, if we define
\begin{equation*}
Y^{(b)}_n(x) = \sqrt{n}(F^{(b)}_n(x) - F_n(x)) - 
\sqrt{n}(F(x; \hat\theta^{(b)}_n) - F(x; \hat\theta_n)) 
= \sqrt{n}(F^{(b)}_n(x) - F(x; \hat\theta^{(b)}_n)) - 
\sqrt{n}(F_n(x) - F(x; \hat\theta_n)),
\end{equation*}
then $T^{(b)}_n := \sup_x|Y^{(b)}_n(x)|$ is the bootstrap statistic that is expected
to approximate the distribution of $T_n$. We note that the term
$\sqrt{n}(F_n(x) - F(x; \hat\theta_n))$ is exactly the bias term considered in 
Babu and Rao (2004).\jy{cite}


We now consider the case where $X_i$'s are realizations from a time series and
$X^{(b)}_1,...,X^{(b)}_n$ are generated by block bootstrap for 
$b \in \{1, \ldots, B\}$. Let $E^{(b)}$ refer to the expectation with respect to
the bootstrap distribution. In this case, we can 
approximate the distribution of
$\sqrt{n}(F_n(x) - F(x))$
by that of
$\sqrt{n}(F^{(b)}_n(x) - E^{(b)}[F^{(b)}_n(x)])$,
\jy{change the languages to add variety}
and the distribution of $\sqrt{n}(F(x; \theta_n) - F(x))$
by that of $\sqrt{n}(F(x; \hat\theta^{(b)}_n) - F(x; E^*[\hat\theta^{(b)}_n]))$.


We can compute $E^{(b)}[F^{(b)}_n(x)]$ and 
$E^{(b)}[\hat\theta^{(b)}_n]$ numerically (they can also be computed analytically, 
depending on the types of block bootstrap we use). One can compute 
$F^{(b)}_n$ 
and $\hat\theta^{(b)}_n$ based on
the $b$th bootstrap sample. Then
$E^{(b)}[F^{(b)}_n(x)] \approx \frac{1}{B}\sum_{b = 1}^BF^{(b)}_n(x)$, and
$E^{(b)}[\hat\theta^{(b)}_n] \approx \frac{1}{B}\sum_{b = 1}^B\hat\theta^{(b)}_n$.
In this case, we can define
\begin{align*}
  Y^*_n(x) &= \sqrt{n}(F^{(b)}_n(x) - E^{(b)}[F^{(b)}_n(x)]) - 
             \sqrt{n}(F(x; \hat\theta^{(b)}_n) - F(x; E^{(b)}[\hat\theta^{(b)}_n)]) \\
           &= \sqrt{n}(F^{(b)}_n(x) - F(x; \hat\theta^{(b)}_n)) -
             \sqrt{n}(E^{(b)}[F^{(b)}_n(x)] - F(x; E^{(b)}[\hat\theta^{(b)}_n])),
\end{align*}
and $T^{(b)}_n = \sup_x|Y^{(b)}_n(x)|$.

\jy{Summary the procedure in an algorithm.}

The p-value of the block bootstrap KS test can be computed
\jy{not right}
as $p = \#\{T^{(b)}_n < T_n\} / B$ for 
$b \in \{1, \ldots, B\}$.

\section{Simulation Studies}
\label{sec:simu}

\paragraph{Size}
We first must evaluate whether this method works when $H_0$ is true. To
test this, we can
generate a simulated sample $X_t$ from a certain distribution $F(\theta)$,
and use the block bootstrap KS test to test if $X_t$ is generated from 
distribution $F$ with some unknown $\theta$. If the test holds its size, the 
p-value
of the test should be uniformally distributed between 0 and 1. We must try the
method with different marginal distributions to ensure that it is robust.
The results of the method depend on the sample size, so we must find the minimum
sample size required for the block bootstrap KS test to work.
We can implement the procedure described in methods with the true distribution
$F ~ N$, hypothesize distribution $F_0 ~ N$, and true mean $\mu = 8$ and 
standard deviation $\sigma = \sqrt{8}$.
We replicate this procedure 1000 times to get the distribution of the p-values 
for the test when applied to samples from the same data generating process.
Then, we repeat the same procedure with $F ~ \Gamma$, and true shape $k = 8$ and
scale $\theta = 1$.

We constructed Q-Q plots of the distribution of the p-values to see if they are
uniformly distributed. If the distribution of the p-values is uniform, this 
indicates that the time series follows the null distribution. 
A zoomed in plot for probabilities between 0 and 0.1 is
also provided, as
that is the most common range for significance levels $\alpha$. 
%We can also
%compute the rate of rejection $\#P < \alpha$ for different values of
%$\alpha$, as well as a confidence interval
%for this proportion.

\begin{figure}[tbp]
  \centering
  \includegraphics[scale=1]{figures/normal}
  \caption{A Q-Q plot of the p-values testing that a distribution
  generated from a $N(8,8)$ data generating process is normal.}
  \label{fig:mu}
\end{figure}

\begin{figure}[tbp]
  \centering
  \includegraphics[scale=1]{figures/zoom_normal}
  \caption{A version of the above plot zoomed in to probabilities between 0 and
  0.1}
  \label{fig:mu}
\end{figure}

\begin{figure}[tbp]
  \centering
  \includegraphics[scale=1]{figures/gamma}
  \caption{A Q-Q plot of the p-values testing that a distribution
  generated from a $\gamma(8,1)$ data generating process is gamma distributed.}
  \label{fig:mu}
\end{figure}

\begin{figure}[tbp]
  \centering
  \includegraphics[scale=1]{figures/zoom_gamma}
  \caption{A version of the above plot zoomed in to probabilities between 0 and
  0.1}
  \label{fig:mu}
\end{figure}


\paragraph{Power}
We must also evaluate if this method works when $H_0$ is false. We can use
the block bootstrap KS test to test if $X_t$ is generated from some other 
distribution $G$ with some unknown $\theta$. In this scenario, if the test is 
indeed powerful,
we would ideally want $\beta$, or the probability of rejection to be 1, but we
expect it to be generally high. Again, we must try the method with different
marginal distributions to ensure that it is robust.
We would then expect the distribution of p-values to be non-uniform, and the rate
of $\#P < \alpha$ to be high, meaning a higher density of low p-values.

If we can show that the test correctly fails to reject $H_0$ under the null
distribution, and correctly rejects $H_0$ for an alternate distribution, we can
state that the method works.

We can find the rate of rejection for  different values for the significance 
level $\alpha$. Additionally, we can construct a 95\% confidence interval for 
this rate. Below are rejection rates for $\phi = 0.2$ and $n = 800$, when
applied to the normal and gamma distributions using $\alpha =$ 0.01, 0.05, and 
0.10.

\input{tables/rr}

As shown in the above table, the rejection rates for these settings are very 
high (almost 1), indicating that the test is very powerful for these settings.

\section{Real Data Analysis}

\jy{Find some real time series to test for their marginal distributions. Open
  data, for example, temperatures or stock returns or ...}

\subsection{Precipitation in Mansfield Connecticut}
Monthly maximums of daily precipiation in inches time series data was retrieved 
from the John F. Kennedy (JFK) International Airport station from the 
National Oceanic and Atmospheric Administration.
%\mc{https://hdsc.nws.noaa.gov/pfds/pfds_map_cont.html?bkmrk=ct}

\subsection{Microsoft Stock Returns}

%\mc{https://www.nasdaq.com/market-activity/stocks/msft/historical}

We expect the distribution of Microsoft stock returns to have a slightly
heavier tail than the normal distribution. We can test that it is normally
distributed (which we expect to reject). Then we can try testing if it follows
the $t$ distribution with different degrees of freedoms.


\section{Concluding Remarks}
\label{sec:conclusion}

We have shown that given a large enough sample of a time series, the block 
bootstrap KS test can appropriately fail to reject the null hypothesis when the
series follows the null distribution. It requires a larger sample size than 
basic bootstrap.





\bibliographystyle{chicago}
\bibliography{citations}


\end{document}
%%% LocalWords: nonparametric semiparametric autocorrelation ARMA
%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% ispell-personal-dictionary: ".aspell.en.pws"
%%% fill-column: 80
%%% eval: (auto-fill-mode 1)
%%% End: