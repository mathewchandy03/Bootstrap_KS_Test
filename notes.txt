8/19 Notes
- Try with 50

6/11 Notes
- Clarify that all replicates and all settings are used
- Highlight more deviation from the line
- Perhaps try min{hat l, sqrt{n}}
- Poor size is because of overly large block size

5/22 Notes
- Ask Dr. Zhang to write a paragraph addressing two comments
- Relative frequency, facet by sample size, horizontal axis is tau
- Pick one setting where block sizes are not too big, sample size 800, tau = 0.5
- Compare performance show whether it holds its size and is powerful
- Compare distribution of block sizes for this one setting, include line
representing our cubic root of sample size to compare

4/25/25 Notes
- Record frequency that block size is larger than quarter/sqrt
- Cite politis book in discussion
- Selection of block size is worth more investigation could be a future direction
- Refer to books in discussion

6/18 Notes
- Two affiliations on paper
- Write about HPC
- Write what each file does
- Try AJUR for Arxiv
- Send GitHub to Dr. Zhang
- Series follow the conditions stated in Kunsch

6/11 Notes
- Two propositions, sketch of proofs is given in supplement
- In main paper, state two propositions

5/7 Notes
- Rename Nonparametric and Parametric
- Put Our Method next to Semiparametric
- Remove SP100
- remove row numbers
- 0.10 instead of 0.1
- Real Data Analysis, report first lag tau
- Remove # 
- Specify nominal significance level of 0.05
- Cite zeimbakis

4/23 Notes
- Remove -0.75 and 0.75 from power curve
- percentage 3 digits
- agreement between empirical size and nominal size improves as sample size increases
- 0.75, agreement does not get there
- Approach is working for tau between -0.25 and 0.25 for 100,
- Larger sample size needed (400 close enough) for tau between -0.5 and 0.5
- weak dependence, method works
- stronger dependence
- usually tau is less than 0.5, then the method can be recommended with a smallish sample size

4/9 Notes
- Try larger block size for -0.75 and 0.75
- Replace zoomed in figure with rejection rate table
- Remove extra space
- This is still the testing statistic, but the distribution under dependence cannot be used
- Anthony's method uses a copula which could affect the result if its too far away from the truth
- This is why the non-parametric method is motivated
- Try semi-parametric bootstrap
- Treat F as an experimental factor
- Say p-values are not uniformly distributed for 0.75 and -0.75 (stronger dependence)
- Take out beta
- Talk about how different experimental factors are associated with rejection rate
- Define block size can depend on temporal dependence level, search literature if adaptive block
size has been developed

4/2 Notes
- Merge tables
- Merge sections
- Make sure digits are the same
- Go over references
- Identify gap in abstract
- For serially dependent data with no specified parameters, non-parametric
bootstrap bias correction does not exist yet
- We propose instead of demonstrate


3/26 Notes
- 3 decimal places
- email

3/19 Notes
- mention references were method was abused
- mention financial data in introduction
- in discussion, future study on comparing two serially dependent series
- polish up methods
- split methods into sections, standard semi parametric, and new development
- rephrase section 3 intro
- when null is true, the test rejects the null at the desired level
- generate series of length million, transform to gamma, look at sample
autocorrelation
- label as empirical quantile, and theoretical quantile

3/5 Notes
- SP100 and SP500 for 4 years include in the paper
- Try different locations in those cities
- Try precipitation data from Central Park
- Try snowfall

2/20 Notes
- Try no resid 
- Try annual maximums of daily precipitation 
- Annual maximum temperature 
- txx (highest day temperature)
- lowest day temperature
- highest night temperature
- lowest night temperature


2/13 Notes
- Try SP500 and SP100
- Try GEV

2/6 Notes
- expand range to 0.11 but keep tick at 0.1
- say that the correlation for 0.75 is already very high (write the number)
- try procedures from Anthony's paper, parametric and non-parametric
- show that if there really is dependence in the series, results will be different
- do same comparisons on microsoft data
- look at sp500 index, tseries package
- make function load from tseries then create a table
- move step five to the beginning (repeat steps for 1 to uppercase B)
- three digits for phi

1/30 Notes
- Mention phi in text
- The two distributions are very similar, and their first
two moments match. 
- Because they are hard to distinguish
- Test is performing conservatively
- Make sure ticks do not overlap
- Connect lines and overlay on same plot

9/27 Notes
- Make sure that method works under null hypothesis
- Add manuscript introduction, method with equations

10/4 Notes
- Follow Anthony's procedure with block bootstrap
- Review Babu and Rao
- May need large sample size

10/11 Notes
- Try larger sample size
- Try with dependent data
- Begin writing paper (review notes)
- Methods

10/18 Notes
- For different alphas, look at empirical level and nominal level
- Zoomed in plot or histograms
- Try 800
- Look at annual precipitation in Mansfield
- Financial time series data (Stock returns of Microsoft)
- Daily closing price
- tseries R package

10/25 Notes
- Confidence interval for rate of rejection
- Try n = 3200
- Test that it's from normal distribution (data is from t distribution with certain dfs)
- hist.quote
- Microsoft data has heavier tail than normal, so test that it is normal (which we expect to reject)

11/8 Notes
- https://www.ncei.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt
- Test power

11/29 Notes
- Try smaller durations for microsoft
- log difference
- Try t
- Rerun simulation with more jobs
- Look if there is more data
- Try other locations for precipitation

1/23 Notes
- Just rejection rate (percentage) 3 digit
- Function of phi, facet by other parameters
- Try -0.8 and 0.8
- Fix summary
- Kendall's tau
- n = 100 to n = 800
- show that there is autocorrelation (try autocorrelation and autocorrelation of squares)
- fit arma model and look at residuals, try t fit on residuals